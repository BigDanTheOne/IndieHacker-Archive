# How I studied LLMs in two weeks - A comprehensive roadmap

**Author:** Hesam Sheikh
**Date:** November 8, 2024
**Source:** https://www.indiehackers.com/post/nAE5IgarUrnzNHGizEZX

---

Understanding Large Language Models requires going deeper into fundamental concepts. The author shares a structured 14-day learning journey covering three main stages:

## Stage 1: Building an LLM from Scratch
Foundation topics include token and positional embeddings, self-attention mechanisms, transformer architectures, and the "Attention is All You Need" paper. Key resource: "Build a Large Language Model (From Scratch)" by Sebastian Raschka.

## Stage 2: LLM Hallucination
This section explores why language models generate inaccurate information, covering positional bias, exposure bias, and how data, training, and inference phases contribute to hallucination issues.

## Stage 3: Advanced Techniques Beyond Attention
Topics include pause tokens, infini-attention for extended context windows, RoPE (Rotary Positional Embedding), KV Cache optimization, and Mixture of Experts architecture. The author studied Meta's Llama as a practical implementation example.

## Prerequisites Identified
- Mathematics: Linear algebra, probability/statistics, calculus, optimization
- Programming: Python, NumPy, Pandas, PyTorch or TensorFlow
- Deep Learning Concepts: Neural networks, backpropagation, CNNs (optional)

## Key Lessons
1. "Learning is a very personal experience" -- don't rigidly follow established roadmaps; adapt based on your knowledge gaps and interests.
2. Identify specific questions before consuming material rather than attempting complete coverage of resources.
3. Focus on enjoying discovery rather than meeting arbitrary completion timelines.
4. Prioritize understanding attention mechanisms as the core foundation of LLM functionality.

## Learning Resources Cited
- "Build a Large Language Model (From Scratch)" by Sebastian Raschka
- Andrej Karpathy's YouTube playlist on language modeling
- GitHub repository: ml-retreat (330+ stars)

**Tech Stack Mentioned:**
- Python, PyTorch / TensorFlow, NumPy, Pandas
