# The largest ever copyright-free AI training dataset just dropped

**Author:** Katie Hignett
**Date:** November 19, 2024
**Source:** https://www.indiehackers.com/post/DXzoYMyLokUU7gmOI1LG

---

AI company Pleias has released Common Corpus, a massive copyright-free dataset for training language models. The dataset contains just over two trillion tokens sourced from financial and legal documents, open-source code, academic journals, and public domain publications.

Common Corpus is substantially larger than the dataset used for GPT-3 (approximately 300 billion tokens) but smaller than GPT-4's training data (approximately 13 trillion tokens). Every token in the dataset is documented, traceable, and licensed permissibly.

The dataset comprises five categories: Open Culture (public domain books, newspapers, Wikisource content), Open Government (financial and legal documents), Open Source (high-quality code from Github), Open Science (scientific articles with preserved tables and charts), and OpenWeb (Wikipedia, YouTube Commons).

Though predominantly English, the dataset includes substantial French and German language content. Pleias contends this demonstrates that powerful language models can be trained without copyrighted material, challenging claims from major AI firms like OpenAI.

The dataset is available free via HuggingFace as part of the AI Alliance Open Trusted Data Initiative.

**Key Lessons:**
- Copyright-free AI training at scale is achievable
- Transparent data provenance builds regulatory compliance
- Open datasets can support competitive model performance

**Tech Stack Mentioned:** Github, HuggingFace, GPT-3, GPT-4
