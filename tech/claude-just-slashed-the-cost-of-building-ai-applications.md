# Claude just slashed the cost of building AI applications

**Author:** Darko Gjorgjievski
**Date:** August 16, 2024
**Source:** https://www.indiehackers.com/post/lQHHc1S9NLLiPLJeyUWk

---

Claude's new Prompt Caching feature allows developers to reuse text across multiple prompts by caching static content like examples and only sending new variable content. This can reduce input API costs by up to 90%.

Developers building AI SaaS applications often rely on lengthy prompts containing multiple examples to improve output quality. Since API providers charge per input token, these comprehensive prompts drive up costs significantly.

The feature benefits use cases including AI assistants with shared prompts, code generation with reusable templates, code reviews involving lengthy files, document processing, search tools, and any application with example-heavy prompts.

## Key Lessons

- Developers can either lower pricing or increase profit margins through this cost reduction
- The technology removes pressure to optimize prompt length, enabling more thorough instructions
- Practical applications span multiple development domains beyond simple chatbots

**Tech Stack:** Claude (Anthropic's AI model), OpenAI and Google APIs (competitors)
